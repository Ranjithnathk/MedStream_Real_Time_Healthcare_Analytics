{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebdd5a18-0d48-44c2-a458-e729888e4a34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver streaming query started. ID: b3397b4e-62b9-4b33-b5f7-897f0d747ea1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    StringType, IntegerType, DoubleType, TimestampType\n",
    ")\n",
    "from pyspark.sql.functions import (\n",
    "    col, from_json, to_timestamp, when,\n",
    "    current_timestamp, floor, rand, expr\n",
    ")\n",
    "\n",
    "# ADLS Gen2 configuration\n",
    "\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.healthcarestoragerk.dfs.core.windows.net\",\n",
    "    \"xxxxx\"\n",
    ")\n",
    "\n",
    "# Bronze (source) and Silver (target) paths\n",
    "bronze_path = \"abfss://bronze@healthcarestoragerk.dfs.core.windows.net/encounters_raw\"\n",
    "silver_path = \"abfss://silver@healthcarestoragerk.dfs.core.windows.net/encounters_clean\"\n",
    "silver_checkpoint_path = \"abfss://silver@healthcarestoragerk.dfs.core.windows.net/_checkpoints/encounters_clean\"\n",
    "\n",
    "\n",
    "# Read streaming data from Bronze Delta\n",
    "\n",
    "bronze_df = (\n",
    "    spark.readStream\n",
    "         .format(\"delta\")\n",
    "         .load(bronze_path)\n",
    ")\n",
    "\n",
    "\n",
    "# Define schema matching our event JSON\n",
    "\n",
    "encounter_schema = StructType([\n",
    "    StructField(\"encounter_id\",        StringType(), True),\n",
    "    StructField(\"patient_id\",          StringType(), True),\n",
    "    StructField(\"gender\",              StringType(), True),\n",
    "    StructField(\"age\",                 IntegerType(), True),\n",
    "    StructField(\"department\",          StringType(), True),\n",
    "    StructField(\"admission_time\",      StringType(), True),   # will cast to timestamp\n",
    "    StructField(\"discharge_time\",      StringType(), True),   # will cast to timestamp\n",
    "    StructField(\"organization_id\",     StringType(), True),\n",
    "    StructField(\"provider_id\",         StringType(), True),\n",
    "    StructField(\"payer_id\",            StringType(), True),\n",
    "    StructField(\"base_encounter_cost\", DoubleType(), True),\n",
    "    StructField(\"total_claim_cost\",    DoubleType(), True),\n",
    "    StructField(\"payer_coverage\",      DoubleType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Parse JSON and project columns\n",
    "\n",
    "parsed_df = (\n",
    "    bronze_df\n",
    "    .withColumn(\"data\", from_json(col(\"raw_json\"), encounter_schema))\n",
    "    .select(\"data.*\")\n",
    ")\n",
    "\n",
    "\n",
    "# Cast admission/discharge to timestamp\n",
    "\n",
    "clean_df = (\n",
    "    parsed_df\n",
    "    .withColumn(\"admission_time\", to_timestamp(\"admission_time\"))\n",
    "    .withColumn(\"discharge_time\", to_timestamp(\"discharge_time\"))\n",
    ")\n",
    "\n",
    "\n",
    "# Handle invalid / future admission_time\n",
    "# - if null or in the future -> current_timestamp()\n",
    "\n",
    "clean_df = clean_df.withColumn(\n",
    "    \"admission_time\",\n",
    "    when(\n",
    "        col(\"admission_time\").isNull() | (col(\"admission_time\") > current_timestamp()),\n",
    "        current_timestamp()\n",
    "    ).otherwise(col(\"admission_time\"))\n",
    ")\n",
    "\n",
    "\n",
    "# Handle invalid discharge_time\n",
    "# - if null or before admission_time -> admission_time + 15 minutes\n",
    "\n",
    "clean_df = clean_df.withColumn(\n",
    "    \"discharge_time\",\n",
    "    when(\n",
    "        col(\"discharge_time\").isNull() | (col(\"discharge_time\") < col(\"admission_time\")),\n",
    "        col(\"admission_time\") + expr(\"INTERVAL 15 MINUTES\")\n",
    "    ).otherwise(col(\"discharge_time\"))\n",
    ")\n",
    "\n",
    "\n",
    "# Handle invalid age\n",
    "# - if age < 0 or > 110 -> random age between 1 and 90\n",
    "# (this fixes weird 150+ ages from bad birthdates)\n",
    "\n",
    "clean_df = clean_df.withColumn(\n",
    "    \"age\",\n",
    "    when((col(\"age\") < 0) | (col(\"age\") > 110),\n",
    "         floor(rand() * 90 + 1).cast(\"int\"))\n",
    "    .otherwise(col(\"age\"))\n",
    ")\n",
    "\n",
    "\n",
    "# Derive encounter duration (minutes)\n",
    "\n",
    "clean_df = clean_df.withColumn(\n",
    "    \"encounter_duration_minutes\",\n",
    "    (\n",
    "        col(\"discharge_time\").cast(\"long\") - col(\"admission_time\").cast(\"long\")\n",
    "    ) / 60.0\n",
    ")\n",
    "\n",
    "\n",
    "# Schema evolution safety:\n",
    "# ensure all expected columns exist\n",
    "\n",
    "expected_cols = [\n",
    "    \"encounter_id\",\n",
    "    \"patient_id\",\n",
    "    \"gender\",\n",
    "    \"age\",\n",
    "    \"department\",\n",
    "    \"admission_time\",\n",
    "    \"discharge_time\",\n",
    "    \"organization_id\",\n",
    "    \"provider_id\",\n",
    "    \"payer_id\",\n",
    "    \"base_encounter_cost\",\n",
    "    \"total_claim_cost\",\n",
    "    \"payer_coverage\",\n",
    "    \"encounter_duration_minutes\"\n",
    "]\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "for col_name in expected_cols:\n",
    "    if col_name not in clean_df.columns:\n",
    "        clean_df = clean_df.withColumn(col_name, lit(None))\n",
    "\n",
    "# Re-order columns for consistency\n",
    "clean_df = clean_df.select(*expected_cols)\n",
    "\n",
    "\n",
    "# Write to Silver Delta (streaming)\n",
    "\n",
    "silver_query = (\n",
    "    clean_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .option(\"checkpointLocation\", silver_checkpoint_path)\n",
    "    .start(silver_path)\n",
    ")\n",
    "\n",
    "print(\"Silver streaming query started. ID:\", silver_query.id)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_silver_cleandata",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}